A relaxação Lagrangeana é um modo de aumentar o conjunto de soluções viáveis de um programa linear retirando uma de suas restrições e penalizando soluções que não respeitem essa restrição. Para o programa linear que é uma relaxação do programa inteiro do $k$-mediana iremos retirar a restrição que limita a quantidade de instalações abertas. Essa remoção resultará no programa
\begin{align}
    \text{Minimizar} \quad & \sum_{i \in F, j \in D} c_{ij}x_{ij} + \lambda \left(\sum_{i \in F} (y_i) - k\right) \\
    \text{sujeito a} \quad & \sum_{i \in F} x_{ij} \geq 1, &&\forall j \in D, \\
                           & y_i - x_{ij} \geq 0, &&\forall i \in F, j \in D, \\
                           & x_{ij} \geq 0, && \forall i \in F,j \in D, \\
                           & y_i \geq 0, &&\forall i \in F,
\end{align}
em que $\lambda$ é um multiplicador de Lagrange que nos deixará escolher o quão importante o desconto pode ser para o valor objetivo. Para um $\lambda \geq 0$, qualquer solução viável da relaxação linear do $k$-mediana tem valor objetivo nesse programa no máximo o valor objetivo que ele tinha anteriormente. Assim, o valor ótimo da relaxação Lagrangeana é no máximo o valor ótimo da relaxação linear do problema $k$-mediana.

Note que uma solução que somente não respeitava a restrição $\sum_{i \in F} y_i \leq k$, agora é uma solução viável, porém com desconto no valor objetivo, uma vez que $\sum_{i \in F}(y_i) - k $ será estritamente positivo e no nosso problema queremos minimizar o valor objetivo.

Como essa relaxação é um programa linear, podemos encontrar o seu dual. O dual desse programa é
\begin{align}
    \text{Minimizar} \quad & \sum_{j \in D} (v_j) - \lambda k \\
    \text{sujeito a} \quad & v_j - w_{ij} \leq c_{ij}, &&\forall i \in F, j\in D, \\
                           & \sum_{j\in D} w_{ij} \leq \lambda, &&\forall i \in F, \\
                           & v_j \geq 0, &&\forall j\in D, \\
                           & w_{ij} \geq 0, && \forall i \in F,j \in D.
\end{align}

Observe que, com exceção dos termos constantes $-\lambda k$ nos valores objetivos, os dois programas lineares são idênticos a uma instância da relaxação linear do problema de localização de instalações e seu dual em que cada facilidade tem custo de abertura $f_i = \lambda$. Desse modo, gostaríamos de usar algum algoritmo para o problema localização de instalações para encontrar uma solução para o problema $k$-mediana. No algoritmo de dual fitting da seção~\ref{facility:guloso&fitting} encontramos um conjunto $X$ de instalações e construímos variáveis $\alpha$ tal que definindo $v_j \coloneqq \frac{\alpha_j}{2}$ e $w_{ij} \coloneqq (v_j - c_{ij})_+$ para toda instalação $i$ e cliente~$j$ vale que $(v, w)$ é solução viável do dual e, pelo Teorema~\ref{greedy:5}, vale que
\[\sum_{j \in D} c(j,X) + 2\sum_{i \in X} f_i = \sum_{j \in D} \alpha_j =  2\sum_{j\in D} v_j.\]
Substituindo $f_i$ por $\lambda$ e rearranjando, encontramos que
\[\sum_{j\in D} c(j,X) \leq 2\left(\sum_{j\in D}v_j - \lambda|X|\right).\]
Se $|X| = k$, então esse algoritmo é uma $2$-aproximação para o problema $k$-mediana, uma vez que $\left( \sum_{j\in D} v_j - \lambda k\right)$ é o valor da função objetivo do dual da relaxação Lagrangeana que por sua vez é um limitante inferior da resposta ótima para o problema $k$-mediana. 

Note que o valor de $\lambda$ está diretamente relacionado com a quantidade de instalações que serão abertas, caso $\lambda = 0$ é normal esperar que muitas instalações sejam abertas e caso $\lambda = \sum_{j \in D}\sum_{i \in F}c_{ij}$ o algoritmo abrirá apenas uma instalação. Então é natural pensar em encontrar um método que garanta um $\lambda$ que abra exatamente $k$ instalações, porém tal método não é conhecido. No entanto, podemos rodar uma busca binária para tentar encontrá-lo. Nessa busca binária, manteremos dois valores de $\lambda$, o primeira se iniciará como $\lambda_1 \coloneqq 0$ e o segundo como $\lambda_2 \coloneqq \sum_{j \in D}\sum_{i \in F}c_{ij}$, esses dois valores de $\lambda$ retornam soluções $X_1$ e $X_2$ (respectivamente) com $|X_1| > k$ e $|X_2| < k$. Rodaremos o algoritmo com o valor $\lambda \coloneqq \frac{1}{2}\left(\lambda_1 + \lambda_2 \right)$ que retorna um conjunto de instalações $X$. Caso $|X| = k$, então encontramos um $\lambda$ que garante uma $2$-aproximação. Caso contrário, atualizaremos o valor de $\lambda_1$ ou de $\lambda_2$, caso $|X| > k$ então atualizaremos o valor de $\lambda_1$ e, caso contrário, atualizaremos o valor de $\lambda_2$. Assim, rodaremos esse algoritmo até encontrar um $\lambda$ que garanta a abertura de $k$ instalações ou até que a diferença entre $\lambda_1$ e $\lambda_2$ seja tão pequena que seremos capazes de encontrar uma solução para o problema $k$-mediana combinando essas duas soluções. Considerando $c_{\min}$ como o valor da menor aresta não nula, o valor que vamos considerar como suficiente para pararmos a busca binária é quando $\lambda_2 - \lambda_1 \leq \frac{\epsilon c_{\min}}{|F|}$. Vamos assumir aqui que $0 < c_{\min} \leq opt(I)$ e vamos mostrar que é possível encontrar resposta para o problema $k$-mediana em tempo polinomial quando $opt(I)=0$ \red{PROVAR ISSO, EXERCICIO 7.9 DO WS}. Quando o algoritmo parar por conta dessa diferença, iremos combinar as soluções $X_1$ e $X_2$ para encontrar uma solução viável $X$ para o problema $k$-mediana com custo no máximo $4 + \epsilon$ vezes o ótimo.

Note que essa busca binária em $\lambda$ faz $O(\log \frac{|F| \sum c_{ij}}{\epsilon c_{\min}})$ chamadas ao algoritmo dual fitting do problema de localização de instalações e, por isso, leva tempo polinomial para ser executado.

A partir desse momento, vamos considerar que estamos em uma instância que não foi possível encontrar um $\lambda$ que nos resultasse em uma solução $X$ com $|X| = k$. Então, temos $\lambda_1$ e $\lambda_2$ que nos levam a soluções $X_1$ e $X_2$ com $|X_1| > k$ e $|X_2| < k$, respectivamente, com $\lambda_2 - \lambda_1 \leq \frac{\epsilon c_{\min}}{|F|}$. Vamos considerar também as variáveis $\alpha^1$ e $\alpha^2$ produzidas pelo algoritmo de dual fitting e definir $v_j^\ell \coloneqq \alpha_j^\ell$ e $w_{ij}^\ell \coloneqq (v_j^\ell - c_{ij})_+$ para instalações $i$ e clientes $j$ e para $\ell = 1,2$. Vamos, então, construir a solução prometida. Seja $a$ e $b$ tal que $a|X_1| + b|X_2| = k$ e $a + b = 1$ com $a,b>0$, ou seja, coeficientes de uma combinação linear com os tamanhos das soluções encontradas que resulta em $k$. Esses coeficientes existem, uma vez que $|X_1| > k > |X_2|$. Isso implica que 
\[ a = \frac{k - |X_2|}{|X_1| - |X_2|} \ \text{ e }\ b = \frac{|X_1| - k}{|X_1| - |X_2|}.\]
Conseguimos construir uma solução do dual $(v,w)$ definindo $v = av^1 + bv^2$ e $w~=~aw^1~+~bw^2$. É fácil notar que essa solução é viável quando utilizamos custos de abertura $\lambda_2$, uma vez que $(v^1,w^1)$ também é solução viável do dual quando utilizamos $\lambda_2$ e, portanto, $(v,w)$ é combinação convexa de duas soluções viáveis. Seja $I$ a instância do problema $k$-mediana recebido. Vamos provar o lema 
\begin{lemma}
    \label{k-median_relLag_lema1}
    \[ac(X_1) + bc(X_2) \leq (1+\epsilon) 2\opt(I).\]
\end{lemma}
\begin{proof}
Primeiro observemos que 
\begin{subequations}
    \begin{align*}
        c\left(X_1\right) &\leq 2\left(\sum_{j \in D} v_j^1 - \lambda_1|X_1|\right) = 2\left(\sum_{j\in D} v_j^1 - \left( \lambda_1 + \lambda_2 - \lambda_2 \right)|X_1|\right)\\
        & = 2 \left(\sum_{j \in D} v_j^1 - \lambda_2 |X_1| \right) + 2 \left(\lambda_2 - \lambda_1\right) |X_1| \\
        &\leq 2 \left(\sum_{j \in D} v_j^1 - \lambda_2 |X_1| \right) + 2\frac{\epsilon\, c_{\min}}{|F|}|X_1| \leq 2 \, \epsilon \, \opt(I)
    \end{align*}
\end{subequations}
onde a primeira desigualdade vale pela garantia do algoritmo utilizado para encontrar a solução $X_1$, a segunda vale pelo limitante imposto para $\lambda_2 - \lambda_1$ e a última vale, pois $c_{\min} \leq \opt(I)$ e $\frac{|X_1|}{|F|} \leq 1$. Utilizando a desigualdade análoga à primeira desigualdade só que para $c(X_2)$ e utilizando o coeficiente convexo para construir a nossa solução, temos
\begin{subequations}
    \begin{align*}
        ac(X_1) + b c(X_2) &\leq 2a \left(\sum_{j\in D} v_j^1 - \lambda_2|X_1|\right) + 2a\, \epsilon\,\opt(I) + 2b\left(\sum_{j\in D} v_j^2 - \lambda_2|X_2|\right) \\
        &= 2\left(\sum_{j \in D} a v_j^1 + b v_j^2\right) - 2\lambda_2\left( a|X_1| + b|X_2|\right) + 2a\,\epsilon\,\opt(I)\\
        & = 2\left(\sum_{j\in D } v_j\right) - 2\lambda_2 k + 2a\,\epsilon\,\opt(I)\\
        & = 2\left( \sum_{j \in D } v_j -\lambda_2 k\right) + 2a\,\epsilon\,\opt(I) \leq (1+\epsilon)2opt(I)
    \end{align*}
\end{subequations}
em que a segunda desigualdade vale pois $(v,w)$ é uma solução viável para o dual com custo de abertura de instalações $\lambda_2$ e porque $a \leq 1$.
\end{proof}

O algoritmo então separa em dois casos, um caso simples em que $b \geq \frac{1}{2}$ e um caso um pouco mais complicado em que $b < \frac{1}{2}$. Se $b \geq \frac{1}{2}$, então devolvemos $X_2$ como solução viável, uma vez que $|X_2| < k$. Usando $b \geq \frac{1}{2}$ e o Lema~\ref{k-median_relLag_lema1}, temos que
\begin{subequations}
    \begin{align*}
        c(X_2) \leq 2bc(X_2) \leq 2\left(ac(X_1) + bc(X_2) \right) \leq 2(1+\epsilon) 2\opt(I) = (1+\epsilon)4\opt(I).
    \end{align*}
\end{subequations}

Agora precisamos mostrar que essa desigualdade também vale para o caso em que $ b < \frac{1}{2}$. Para cada instalação $i \in X_2$, vamos abrir a instalação $h \in X_1$ que minimiza $c_{ih}$. É possível que duas instalações de $X_2$ tenham como instalação mais próxima em $X_1$ a mesma instalação, nesse caso iremos abrir instalações de $X_1$ arbitrariamente até que $|X_2|$ instalações sejam abertas. Após isso, escolhemos aleatoriamente um subconjunto de tamanho $k - |X_2|$ das $|X_1| - |X_2|$ instalações restantes de $X_1$ e abrimos ele. Seja $X$ o conjunto de instalações abertas ao final de todo esse processo. Vamos mostrar o seguinte lema.
\begin{lemma}
    \label{k-median_relLag_lema2}
    Se $b < \frac{1}{2}$, então o custo esperado da solução gerada é no máximo $(1 + \epsilon)4\opt(I)$.
\end{lemma}
\begin{proof}
Para provar esse lema vamos considerar o custo de associação esperado para um dado cliente $j$ para uma instalação aberta pelo algoritmo aleatório. Seja $i_1$ a instalação de $X_1$ mais próxima de $j$ e $i_2$ a instalação de $X_2$ mais próxima de $j$. Note que a probabilidade de $i_1$ ser aberta pelo algoritmo aleatório é $\frac{k - |X_2|}{|X_1| - |X_2|} = a$, caso ela não tenha sido aberta na primeira parte do algoritmo. Então, com probabilidade pelo menos $a$, o custo de associação de $j$ para a instalação mais próxima em $S$ é no máximo $c_{i_1j}$. Com probabilidade no máximo $1 - a = b$, a instalação $i_1$ não será aberta. Nesse caso, no pior dos casos, iremos associar $j$ a uma instalação aberta no primeiro passo do algoritmo, em particular, a instalação de $X_1$ mais próxima à $i_2$. Seja $i$ a instalação  de $X_1$ mais próxima a $i_2$. Então, pela desigualdade triangular, vale que
\[c_{ij} \leq c_{ii_2} + c_{i_2j}.\]
Pela definição de $i$, sabemos que $c_{ii_2} \leq c_{i_1i_2}$. Então \(c_{ij} \leq c_{i_1i_2} + c_{i_2j}\). Novamente pela desigualdade triangular, vale que 
\[c_{i_1i_2} \leq c_{i_1j} + c_{i_2j}\]
e, consequentemente
\[c_{ij} \leq c_{i_1j} + c_{i_2j} + c_{i_2j} = c(j,X_1) + 2 c(j,X_2).\]
Assim, o custo de associação esperado de $j$ para a instalação mais próxima em $X$ é
\[\mathbb{E}[c(j,X)] \leq a c\left(j,X_1\right) + b\left(c(j,X_1) + 2 c(j,X_2)\right) = c(j,X_1) + 2bc(j,X_2).\] 
Como $b < \frac{1}{2}$, então $a = 1 - b > \frac{1}{2}$ e 
\[\mathbb{E}[c(j,X)] \leq 2\left( a c(j,X_1) + bc(j,X_2)\right).\]
Somando para todos os clientes $j$, temos
\begin{subequations}
    \begin{align*}
        \sum_{j \in D}\mathbb{E}[c(j,X)] &\leq \sum_{j \in D}2\left( a c(j,X_1) + bc(j,X_2)\right) = 2\left(a \sum_{j\in D} c(j,X_1) + b \sum_{j \in D} c(j,X_2)\right) \\
        & = 2\left( a c(X_1) + b c(X_2)\right) \leq 2 (1 + \epsilon) 2 \opt(I) = (1+\epsilon) 4\opt(I)
    \end{align*}
\end{subequations}
em que a última desigualdade vale pelo Lema~\ref{k-median_relLag_lema1}.
\end{proof}

\red{AQUI VAI ENTRAR A DERANDOMIZAÇÃO}

Esse método se encontra na seção 7.7 do livro WS2011. Ele é muito semelhante ao método utilizado no capítulo 25 do livro V2001, mas no livro do Vazirani ele utiliza o dual da relaxação linear do problema $k$-median, no lugar de utilizar o dual da relaxação Lagrangeana. A diferença é que no dual da relaxação linear o multiplicador de Lagrange aparece como uma variável do próprio programa, mas ela a sua interpretação e utilização são idênticas. Assim, os algoritmos são, no geral, equivalentes.